<!DOCTYPE html>
<meta charset="utf-8"> 
<html lang="en">
        
<head>
    <title>音频测试</title>
</head>
<body>
<!--<canvas id="canvas"></canvas>-->
<p>
    最大音量： <input  type="text" id="maxAudio"></input>
</p>
<p>
    实时音量：<input   type="text"  id="autoAudio"></input>
</p>

<div id='debug' width="100%">
</div>

</body>

<!--<script src="vudio.js"></script>-->
<script type="text/javascript">
  // var canvas = document.querySelector('#canvas')

  function debug(msg){
      var elem = document.getElementById('debug');
      var p = document.createElement('p');
      p.innerText = msg;
      elem.appendChild(p);
  }

  var maxNum = 0;
  var maxAudio = document.getElementById("maxAudio");
  var autoAudio = document.getElementById("autoAudio");



  // 老的浏览器可能根本没有实现 mediaDevices，所以我们可以先设置一个空的对象
if (navigator.mediaDevices === undefined) {
  navigator.mediaDevices = {};
  debug('navigator.mediaDevices is undefined ! ')
}

// 一些浏览器部分支持 mediaDevices。我们不能直接给对象设置 getUserMedia 
// 因为这样可能会覆盖已有的属性。这里我们只会在没有getUserMedia属性的时候添加它。
if (navigator.mediaDevices.getUserMedia === undefined) {
  navigator.mediaDevices.getUserMedia = function(constraints) {

    // 首先，如果有getUserMedia的话，就获得它
    var getUserMedia = navigator.webkitGetUserMedia || navigator.mozGetUserMedia;

    // 一些浏览器根本没实现它 - 那么就返回一个error到promise的reject来保持一个统一的接口
    if (!getUserMedia) {
      debug('navigator.webkitGetUserMedia || navigator.mozGetUserMedia is not supported on this browser .1');
    }

    // 否则，为老的navigator.getUserMedia方法包裹一个Promise
    return new Promise(function(resolve, reject) {
      getUserMedia.call(navigator, constraints, resolve, reject);
    });
  }
}

  debug(navigator.mediaDevices);

  navigator.mediaDevices.getUserMedia({
    audio: true
  }).then((stream) => {
    debug('getUserMedia ok! ')
      // 1. 创建AudioContext音频对象
      var audioContext = new (window.AudioContext || window.webkitAudioContext || window.mozAudioContext) ();
      // 2. stream参数传入，以实现将麦克风音频输入该AudioContext对象
       var source = Object.prototype.toString.call(stream) !== '[object MediaStream]' ? audioContext.createMediaElementSource(stream) : audioContext.createMediaStreamSource(stream);

      // 3.创建音频分析对象
      // 在该对象中指定采样的缓冲区域大小，该值一般为256、512、1024、2048、4096、8192、16384中的一种，数字越大则采样缓冲区越大，对应的audioprocess事件的触发频率也越低，数字越小则反之，在此将该参数设置为4096
      // 同时，设置音频分析的输入与输出都是 单声道，其参数为1（若要以 双声道 进行分析则可设置为2
      var  scriptNode =  audioContext.createScriptProcessor(4096,1,1);

      // 音频处理函数
      scriptNode.onaudioprocess = function(audioProcessingEvent) {
          var inputBuffer = audioProcessingEvent.inputBuffer;
          // var outputBuffer = audioProcessingEvent.outputBuffer;
          for (var channel = 0; channel < inputBuffer.numberOfChannels; channel++) {
              var inputData = inputBuffer.getChannelData(channel);
              for(var i= 0; i < inputData.length;i++){
                  var num = 10000 * inputData[i];
                  if(maxNum < num){
                      maxNum=num;
                  }
                  autoAudio.value= num;
                  var tmp=maxAudio.value;
                  if(tmp<maxNum){
                      maxAudio.value= maxNum;
                  }
              }

          }
      }
      source.connect(scriptNode);
      scriptNode.connect(audioContext.destination);
      // source.start();

  }).catch((error) => {
    debug('getUserMedia not supported on this browser!')
  })

</script>
</html>
